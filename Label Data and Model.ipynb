{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import os \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import CalibratedClassifierCV \n",
    "pd.set_option('display.max_columns', 999)\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "from numpy import argmax\n",
    "import pickle \n",
    "import threading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 2520\n",
      "Null values after filling: 0\n"
     ]
    }
   ],
   "source": [
    "summaries_df = pd.read_csv('bill_summaries.csv')\n",
    "subjects_df = pd.read_csv('bill_metadata/subjects/comparative_agendas_labeled_bills.csv')\n",
    "merged_df = pd.merge(subjects_df, summaries_df, how='left',\n",
    "              left_on='json_bill_id', right_on='bill_number')\n",
    "df = merged_df[['bill_number', 'summary', 'majortopic', 'subtopic']]\n",
    "X = df['summary'].astype(str)\n",
    "y = df['majortopic'] #.astype(str)\n",
    "print ('Null values:', y.isna().sum())\n",
    "y = y.fillna(404)\n",
    "print ('Null values after filling:', y.isna().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2',\n",
    "                        encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(X)\n",
    "labels = y\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_probs_and_classes(model_scores, model_classes, top_n):\n",
    "    score_number_list = []\n",
    "    prob_list = []\n",
    "    predicted_label_list = []\n",
    "    descending_order = (-model_scores).argsort()\n",
    "    #ordered_scores = descending_order[0][:top_n]\n",
    "    for model_score in range(len(model_scores)):\n",
    "        ordered_scores = descending_order[model_score][:top_n]\n",
    "        for score in ordered_scores:\n",
    "            predicted_label = model_classes[score]\n",
    "            prob = (model_scores[model_score][score]) * 100\n",
    "            prob = \"%.3f\" % round(prob, 3)\n",
    "            prob_list.append(prob)\n",
    "            predicted_label_list.append(predicted_label)\n",
    "            score_number_list.append(model_score)\n",
    "        \n",
    "    data_dict = {'score_number': score_number_list,\n",
    "                 'probability': prob_list,\n",
    "                'predicted_label': predicted_label_list}\n",
    "        \n",
    "    df = pd.DataFrame(data=data_dict)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(known_label_list, prediction_df):\n",
    "    known_labels = known_label_list.reset_index(drop=True)\n",
    "    hits = 0\n",
    "    \n",
    "    for i in range(len(known_labels)):\n",
    "        if known_labels[i] in list(prediction_df['predicted_label'][\n",
    "            prediction_df['score_number'] == i]):\n",
    "            hits +=1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    accuracy = round(((hits / len(known_labels))*100), ndigits=2) \n",
    "    \n",
    "    \n",
    "    #print (accuracy,'%')\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_1 = df[df['majortopic'] == 1]\n",
    "sub_df_2 = df[df['majortopic'] == 2]\n",
    "sub_df_3 = df[df['majortopic'] == 3]\n",
    "sub_df_4 = df[df['majortopic'] == 4]\n",
    "sub_df_5 = df[df['majortopic'] == 5]\n",
    "sub_df_6 = df[df['majortopic'] == 6]\n",
    "sub_df_7 = df[df['majortopic'] == 7]\n",
    "sub_df_8 = df[df['majortopic'] == 8]\n",
    "sub_df_9 = df[df['majortopic'] == 9]\n",
    "sub_df_10 = df[df['majortopic'] == 10]\n",
    "sub_df_12 = df[df['majortopic'] == 12]\n",
    "sub_df_13 = df[df['majortopic'] == 13]\n",
    "sub_df_14 = df[df['majortopic'] == 14]\n",
    "sub_df_15 = df[df['majortopic'] == 15]\n",
    "sub_df_16 = df[df['majortopic'] == 16]\n",
    "sub_df_17 = df[df['majortopic'] == 17]\n",
    "sub_df_18 = df[df['majortopic'] == 18]\n",
    "sub_df_19 = df[df['majortopic'] == 19] \n",
    "sub_df_20 = df[df['majortopic'] == 20]\n",
    "sub_df_21 = df[df['majortopic'] == 21]\n",
    "sub_df_99 = df[df['majortopic'] == 99] \n",
    "sub_df_404 = df[df['majortopic'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sub_dfs = [sub_df_1,\n",
    "                  sub_df_2,\n",
    "                  sub_df_3,\n",
    "                  sub_df_4,\n",
    "                  sub_df_5,\n",
    "                  sub_df_6,\n",
    "                  sub_df_7,\n",
    "                  sub_df_8,\n",
    "                  sub_df_9,\n",
    "                  sub_df_10,\n",
    "                  sub_df_12,\n",
    "                  sub_df_13,\n",
    "                  sub_df_14,\n",
    "                  sub_df_15,\n",
    "                  sub_df_16,\n",
    "                  sub_df_17,\n",
    "                  sub_df_18,\n",
    "                  sub_df_19,\n",
    "                  sub_df_20,\n",
    "                  sub_df_21,\n",
    "                  sub_df_99,\n",
    "                  sub_df_404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in list_of_sub_dfs:\n",
    "    print (l['subtopic'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_404['subtopic'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minor_subject_classifier_eval(summaries_and_labels_df):\n",
    "    \n",
    "    too_few_labels = []\n",
    "    too_few_counts = [] \n",
    "    \n",
    "    unique_labels = list(summaries_and_labels_df['subtopic'].unique())\n",
    "    for label in unique_labels:\n",
    "        \n",
    "        if len(summaries_and_labels_df['subtopic'][summaries_and_labels_df[\n",
    "            'subtopic'] == label])/len(summaries_and_labels_df)*100 > 0.1:\n",
    "            pass\n",
    "        else:\n",
    "            too_few_labels.append(label)\n",
    "            too_few_counts.append(len(summaries_and_labels_df['subtopic'][\n",
    "                summaries_and_labels_df['subtopic'] == label]))\n",
    "            \n",
    "    if len(too_few_labels) > 0:\n",
    "        summaries_and_labels_df = summaries_and_labels_df[~\n",
    "            summaries_and_labels_df['subtopic'].isin(too_few_labels)]\n",
    "    else:\n",
    "        pass  \n",
    "            \n",
    "            \n",
    "    \n",
    "    X = summaries_and_labels_df['summary'].astype(str)\n",
    "    y = summaries_and_labels_df['subtopic'] #.astype(str)\n",
    "    y = y.fillna(404)\n",
    "    \n",
    "    \n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2',\n",
    "                        encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "    \n",
    "    features = tfidf.fit_transform(X)\n",
    "    labels = y \n",
    "\n",
    "    models = [\n",
    "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42),\n",
    "        LinearSVC(),\n",
    "        MultinomialNB(),\n",
    "        LogisticRegression(random_state=42),]\n",
    "\n",
    "    CV = 5\n",
    "    cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "    entries = []\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV,\n",
    "                                    n_jobs=-1, verbose=1)\n",
    "        for fold_idx, accuracy in enumerate(accuracies):\n",
    "            entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "    \n",
    "    error_dict = {'label': too_few_labels, 'count': too_few_counts}\n",
    "    \n",
    "    error_df = pd.DataFrame(error_dict)\n",
    "                                                          \n",
    "    print ('Mean Scores:', cv_df.groupby('model_name').accuracy.mean(),\n",
    "          '\\n'*2, 'Median Scores:', cv_df.groupby('model_name').accuracy.median())\n",
    "    \n",
    "    \n",
    "                                                          \n",
    "    return cv_df \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model_trainer(input_df, label_col, input_list, n, desired_output):\n",
    "    \n",
    "    too_few_labels = []\n",
    "    too_few_counts = [] \n",
    "    \n",
    "    unique_labels = list(input_df[label_col].unique())\n",
    "    if len(unique_labels) > 1:\n",
    "        for label in unique_labels:\n",
    "        \n",
    "            if len(input_df[label_col][input_df[\n",
    "                label_col] == label])/len(input_df)*100 > 0.1:\n",
    "                pass\n",
    "            else:\n",
    "                too_few_labels.append(label)\n",
    "                too_few_counts.append(len(input_df[label_col][\n",
    "                    input_df[label_col] == label]))\n",
    "            \n",
    "        if len(too_few_labels) > 0:\n",
    "            input_df = input_df[~\n",
    "                input_df[label_col].isin(too_few_labels)]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        error_dict = {'label': too_few_labels, 'count': too_few_counts}\n",
    "        error_df = pd.DataFrame(error_dict)\n",
    "    \n",
    "        X = input_df['summary'].astype(str)\n",
    "        y = input_df[label_col] #.astype(str)\n",
    "        y = y.fillna(404)\n",
    "    \n",
    "    \n",
    "    \n",
    "        tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                                min_df=5,\n",
    "                                norm='l2',\n",
    "                                encoding='latin-1',\n",
    "                                ngram_range=(1, 2),\n",
    "                                stop_words='english')\n",
    "        if desired_output == 'accuracy_score':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "            model = LinearSVC()\n",
    "            clf = CalibratedClassifierCV(model)\n",
    "            X_train = tfidf.fit_transform(X_train)\n",
    "            X_test = tfidf.transform(X_test)\n",
    "            clf.fit(X_train, y_train)\n",
    "            classes = clf.classes_\n",
    "            scores = clf.predict_proba(X_test)\n",
    "            prediction_df = top_probs_and_classes(model_scores=scores,\n",
    "                                              model_classes=classes,\n",
    "                                              top_n=n)\n",
    "            accuracy_score = top_n_accuracy(y_test, prediction_df)\n",
    "        \n",
    "            print ('\\n'*2, 'Insufficient Scores Report:', '\\n', error_df)\n",
    "            print ('\\n'*2, 'Model Accuracy Scores:', accuracy_score,'%') \n",
    "        else:\n",
    "            model = LinearSVC()\n",
    "            clf = CalibratedClassifierCV(model)\n",
    "            features = input_df['summary'].fillna('No summary available.')\n",
    "            features = tfidf.fit_transform(features)\n",
    "            labels = input_df[label_col]\n",
    "            clf.fit(features, labels)\n",
    "            if desired_output == 'data':\n",
    "                input_list = tfidf.transform(input_list)\n",
    "                classes = clf.classes_\n",
    "                scores = clf.predict_proba(input_list)\n",
    "                prediction_df = top_probs_and_classes(model_scores=scores,\n",
    "                                                model_classes=classes,\n",
    "                                                top_n=n)\n",
    "                print ('\\n'*2, 'Insufficient Scores Report:', '\\n', error_df)\n",
    "                return prediction_df\n",
    "            else:\n",
    "                s = pickle.dumps(clf)\n",
    "                predictor = pickle.loads(s)\n",
    "                print ('\\n'*2, 'Insufficient Scores Report:', '\\n', error_df)\n",
    "                model_dict = {'model':predictor, 'vectorizer': tfidf}\n",
    "                return model_dict\n",
    "    else:\n",
    "        print ('no dice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_list = sub_df_1['summary'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_test = ml_model_trainer(df, 'majortopic', test_input_list, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preliminary_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_every_n(df, n, category):\n",
    "    column_names = []\n",
    "    value_cols = []\n",
    "    for i in range(n):\n",
    "        column_names.append(category + '_label ' + str(i+1))\n",
    "        column_names.append(category + '_prob ' + str(i+1))\n",
    "    for i in range(n):\n",
    "        value_cols.append(df['predicted_label'].iloc[i::n].astype('float'))\n",
    "        value_cols.append(df['probability'].iloc[i::n].astype('float'))\n",
    "    table = pd.DataFrame(value_cols)\n",
    "    transpose = table.transpose()\n",
    "    v = transpose.values\n",
    "    i = np.arange(v.shape[1])\n",
    "    a = np.isnan(v).argsort(0, kind='mergesort')\n",
    "    v[:] = v[a, i] \n",
    "    output_df = transpose\n",
    "    output_df.columns = (column_names)\n",
    "    output_df = output_df.dropna()\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer_test = slice_every_n(preliminary_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slicer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_slice = slicer_test.loc[:0,::2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "column_slice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_slice.head(1).val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " subdfs without multiple unique subtopics: 99, 9, 404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling the subtopic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_list = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_major = ml_model_trainer(df, 'majortopic', empty_list, 3, 'pickle') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1000.0      1\n",
      "1  1001.0      2\n",
      "2  1006.0      1\n",
      "3  1209.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0   321.0      1\n",
      "1   325.0      1\n",
      "2   332.0      1\n",
      "3  2000.0      3\n",
      "4  2004.0      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0   308.0      1\n",
      "1  1209.0      1\n",
      "2     NaN      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      " Empty DataFrame\n",
      "Columns: [label, count]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "    label  count\n",
      "0  302.0      1\n",
      "1  507.0      1\n",
      "2  509.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1612.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "    label  count\n",
      "0  408.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "    label  count\n",
      "0  804.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "    label  count\n",
      "0  710.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0   104.0      1\n",
      "1  2103.0      3\n",
      "2  2104.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0   332.0      1\n",
      "1  1403.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1421.0      1\n",
      "1  1504.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      " Empty DataFrame\n",
      "Columns: [label, count]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1618.0      2\n",
      "1  1621.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1205.0      1\n",
      "1  1702.0      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1801.0      1\n",
      "1  1825.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1916.0      2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0   208.0      1\n",
      "1  2033.0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Insufficient Scores Report: \n",
      "     label  count\n",
      "0  1203.0      1\n",
      "1  2000.0      3\n",
      "2  2001.0      1\n"
     ]
    }
   ],
   "source": [
    "pickle_1 = ml_model_trainer(sub_df_1, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_2 = ml_model_trainer(sub_df_2, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_3 = ml_model_trainer(sub_df_3, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_4 = ml_model_trainer(sub_df_4, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_5 = ml_model_trainer(sub_df_5, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_6 = ml_model_trainer(sub_df_6, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_7 = ml_model_trainer(sub_df_7, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_8 = ml_model_trainer(sub_df_8, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_10 = ml_model_trainer(sub_df_10, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_12 = ml_model_trainer(sub_df_12, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_13 = ml_model_trainer(sub_df_13, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_14 = ml_model_trainer(sub_df_14, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_15 = ml_model_trainer(sub_df_15, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_16 = ml_model_trainer(sub_df_16, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_17 = ml_model_trainer(sub_df_17, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_18 = ml_model_trainer(sub_df_18, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_19 = ml_model_trainer(sub_df_19, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_20 = ml_model_trainer(sub_df_20, 'subtopic', empty_list, 3, 'pickle')\n",
    "pickle_21 = ml_model_trainer(sub_df_21, 'subtopic', empty_list, 3, 'pickle') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_1.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dict = {1: pickle_1, 2: pickle_2, 3: pickle_3, 4: pickle_4,\n",
    "              5: pickle_5, 6: pickle_6, 7: pickle_7, 8: pickle_8,\n",
    "              10: pickle_10, 12: pickle_12, 13: pickle_13, 14: pickle_14,\n",
    "              15: pickle_15, 16: pickle_16, 17: pickle_17, 18: pickle_18,\n",
    "              19: pickle_19, 20: pickle_20, 21: pickle_21}\n",
    "\n",
    "one_subclass_dict = {99:9999, 9:900, 404:404}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_subclass_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summaries_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_subclass_df_dict = {''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def major_label_function(df, n):\n",
    "    tfidf = pickle_major['vectorizer']\n",
    "    clf = pickle_major['model']\n",
    "    #transformed_summaries = tfidf.fit_transform(df['summary'])\n",
    "    transformed_summaries = tfidf.transform(df['summary'])\n",
    "    input_list = transformed_summaries\n",
    "    m_classes = clf.classes_\n",
    "    m_scores = clf.predict_proba(input_list)\n",
    "    m_prediction_df = top_probs_and_classes(model_scores=m_scores,\n",
    "                                            model_classes=m_classes,\n",
    "                                            top_n=n)\n",
    "    m_df = slice_every_n(m_prediction_df, n, 'major')\n",
    "    m_df = pd.concat([df, m_df], axis=1)\n",
    "    return m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_subjects_df = major_label_function(summaries_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(major_subjects_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "major_subjects_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_label_dict = {'major_label': [9, 99, 404],\n",
    "                     'minor_label 1': [900, 9999, 404], 'minor_prob 3': [100, 100, 100],\n",
    "                     'minor_label 2': [900, 9999, 404], 'minor_prob 2': [100, 100, 100],\n",
    "                    'minor_label 3': [900, 9999, 404], 'minor_prob 3': [100, 100, 100]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_label_df = pd.DataFrame(single_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>major_label</th>\n",
       "      <th>minor_label 1</th>\n",
       "      <th>minor_prob 3</th>\n",
       "      <th>minor_label 2</th>\n",
       "      <th>minor_prob 2</th>\n",
       "      <th>minor_label 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>100</td>\n",
       "      <td>900</td>\n",
       "      <td>100</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>9999</td>\n",
       "      <td>100</td>\n",
       "      <td>9999</td>\n",
       "      <td>100</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>100</td>\n",
       "      <td>404</td>\n",
       "      <td>100</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   major_label  minor_label 1  minor_prob 3  minor_label 2  minor_prob 2  \\\n",
       "0            9            900           100            900           100   \n",
       "1           99           9999           100           9999           100   \n",
       "2          404            404           100            404           100   \n",
       "\n",
       "   minor_label 3  \n",
       "0            900  \n",
       "1           9999  \n",
       "2            404  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_dict[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_subjects_df = pd.read_csv('major_subjects_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_label_function(df, n, label_list):\n",
    "    row_list = []\n",
    "    m_labels = label_list\n",
    "    for label in range(len(m_labels)):\n",
    "        if m_labels[label] in one_subclass_dict.keys():\n",
    "            s_df = single_label_df[single_label_df['major_label'] == m_labels[label]]\n",
    "        else:      \n",
    "            model = pickle_dict[m_labels[label]]\n",
    "            tfidf = model['vectorizer']\n",
    "            clf = model['model']\n",
    "            text = [df['summary'][label]] \n",
    "            transformed_summary = tfidf.transform(text)\n",
    "            input_list = transformed_summary\n",
    "            s_classes = clf.classes_\n",
    "            s_scores = clf.predict_proba(input_list)\n",
    "            s_prediction_df = top_probs_and_classes(model_scores=s_scores,\n",
    "                                                  model_classes=s_classes,\n",
    "                                                  top_n=n)\n",
    "            s_df = slice_every_n(s_prediction_df, n, 'minor')\n",
    "        row_list.append(s_df)\n",
    "    output_df = pd.DataFrame(row_list)\n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_sub_label_function(df, n, label_list, output_list):\n",
    "    row_list = []\n",
    "    m_labels = label_list\n",
    "    for label in range(len(m_labels)):\n",
    "        if m_labels[label] in one_subclass_dict.keys():\n",
    "            s_df = single_label_df[single_label_df['major_label'] == m_labels[label]]\n",
    "        else:      \n",
    "            model = pickle_dict[m_labels[label]]\n",
    "            tfidf = model['vectorizer']\n",
    "            clf = model['model']\n",
    "            text = [df['summary'][label]] \n",
    "            transformed_summary = tfidf.transform(text)\n",
    "            input_list = transformed_summary\n",
    "            s_classes = clf.classes_\n",
    "            s_scores = clf.predict_proba(input_list)\n",
    "            s_prediction_df = top_probs_and_classes(model_scores=s_scores,\n",
    "                                                  model_classes=s_classes,\n",
    "                                                  top_n=n)\n",
    "            s_df = slice_every_n(s_prediction_df, n, 'minor')\n",
    "        row_list.append(s_df)\n",
    "    output_df_1 = pd.DataFrame(row_list)\n",
    "    output_df = pd.concat([m_labels, output_df_1], axis=1)\n",
    "    output_list.append(output_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_threading_optimization(df, label_list, n_splits):\n",
    "    list_of_dfs = np.array_split(df, n_splits)\n",
    "    list_of_label_lists = np.array_split(label_list, n_splits)\n",
    "    for i in list_of_dfs:\n",
    "        i.reset_index(drop=True, inplace=True)\n",
    "    for c in list_of_label_lists:\n",
    "        c.reset_index(drop=True, inplace=True)\n",
    "    output_dict = {'sub_dfs': list_of_dfs, 'sub_lists': list_of_label_lists}\n",
    "    return output_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_label_1_subdata = data_threading_optimization(major_subjects_df,\n",
    "                                                   major_label_list,\n",
    "                                                   12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(major_label_1_subdata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(major_label_1_subdata['sub_dfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(major_label_1_subdata['sub_dfs'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_label_1_subdata['sub_dfs'][4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol1 = []\n",
    "ol2 = []\n",
    "ol3 = []\n",
    "ol4 = []\n",
    "ol5 = []\n",
    "ol6 = []\n",
    "ol7 = []\n",
    "ol8 = []\n",
    "ol9 = []\n",
    "ol10 = []\n",
    "ol11 = []\n",
    "ol12 = []\n",
    "ol_list = [ol1, ol2, ol3, ol4, ol5, ol6, ol7, ol8, ol9, ol10, ol11, ol12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ol_list:\n",
    "    print (len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_executor(sub_data_dict, output_lists, start_val):\n",
    "    dfs = sub_data_dict['sub_dfs']\n",
    "    lists = sub_data_dict['sub_lists']\n",
    "    n = 3 \n",
    "    t1 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[0], n,\n",
    "                                lists[0], output_lists[0]))\n",
    "    t2 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[1], n,\n",
    "                                lists[1], output_lists[1]))\n",
    "    t3 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[2], n,\n",
    "                                lists[2], output_lists[2]))\n",
    "    t4 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[3], n,\n",
    "                                lists[3], output_lists[3]))\n",
    "    t5 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[4], n,\n",
    "                                lists[4], output_lists[4]))\n",
    "    t6 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[5], n,\n",
    "                                lists[5], output_lists[5]))\n",
    "    t7 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[6], n,\n",
    "                                lists[6], output_lists[6]))\n",
    "    t8 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[7], n,\n",
    "                                lists[7], output_lists[7]))\n",
    "    t9 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[8], n,\n",
    "                                lists[8], output_lists[8]))\n",
    "    t10 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[9], n,\n",
    "                                lists[9], output_lists[9]))\n",
    "    t11 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[10], n,\n",
    "                                lists[10], output_lists[10]))\n",
    "    t12 = threading.Thread(target=t_sub_label_function,\n",
    "                          args=(dfs[11], n,\n",
    "                                lists[11], output_lists[11]))\n",
    "    if start_val == 1: \n",
    "        t1.start()\n",
    "        t2.start()\n",
    "        t3.start()\n",
    "        t4.start()\n",
    "        t5.start()\n",
    "        t6.start()\n",
    "        t7.start()\n",
    "        t8.start()\n",
    "        t9.start()\n",
    "        t10.start()\n",
    "        t11.start()\n",
    "        t12.start()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_thread_executor = thread_executor(major_label_1_subdata,\n",
    "                                      ol_list,\n",
    "                                      1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_subjects_df.to_csv('major_subjects_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_label_list_1 = major_subjects_df['major_label 1'] \n",
    "major_label_list_2 = major_subjects_df['major_label 2']\n",
    "major_label_list_3 = major_subjects_df['major_label 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subtopic_1_df = sub_label_function(major_subjects_df, 3, major_label_list_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_2_df = sub_label_function(major_subjects_df, 3, major_label_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_3_df = sub_label_function(major_subjects_df, 3, major_label_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_1_df = test_subtopic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_subtopic_df[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_subtopic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_subtopic_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subtopic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concat_test = pd.concat([r for r in test_subtopic_df[0]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(len(major_label_list)):\n",
    "    print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def major_label_function(df, n):\n",
    "    tfidf = pickle_major['vectorizer']\n",
    "    clf = pickle_major['model']\n",
    "    #transformed_summaries = tfidf.fit_transform(df['summary'])\n",
    "    transformed_summaries = tfidf.transform(df['summary'])\n",
    "    input_list = transformed_summaries\n",
    "    m_classes = clf.classes_\n",
    "    m_scores = clf.predict_proba(input_list)\n",
    "    m_prediction_df = top_probs_and_classes(model_scores=m_scores,\n",
    "                                            model_classes=m_classes,\n",
    "                                            top_n=n)\n",
    "    \n",
    "    m_df = slice_every_n(m_prediction_df, n, 'major')\n",
    "    m_df = pd.concat([df, m_df], axis=1)\n",
    "    return m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "        m_labels = m_df.loc[:0,::2].values\n",
    "        \n",
    "        for label in m_labels:\n",
    "            if label in one_subclass_dict.keys():\n",
    "                \n",
    "            model = pickle_dict[label]\n",
    "            input_list = transformed_summaries[bill]\n",
    "            s_classes = model.classes_\n",
    "            s_scores = model.predict_proba(input_list)\n",
    "            s_prediction_df = top_probs_and_classes(model_scores=s_scores,\n",
    "                                              model_classes=s_classes,\n",
    "                                              top_n=n)\n",
    "            s_df = slice_every_n(s_prediction_df, n, 'minor')\n",
    "            m_df =pd.concat([m_df, s_df], axis=1)\n",
    "            \n",
    "        output_df.append(m_df)\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
